# -*- coding: utf-8 -*-
"""Untitled0.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wy8XVJZoNpegqdlOpkgI6klT9rYOD8VE
"""

# fake_news_rf_model.py

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

# 1️⃣ Load Dataset
df = pd.read_excel("fake_news_social_media_6000.xlsx")

# 2️⃣ EDA (Exploratory Data Analysis)
print("Summary Statistics:\n", df.describe())

# Histograms
df.hist(bins=30, figsize=(14, 10))
plt.tight_layout()
plt.savefig("histograms.png")

# Boxplots
plt.figure(figsize=(14, 6))
sns.boxplot(data=df.select_dtypes(include=np.number))
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig("boxplots.png")

# Correlation heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(df.select_dtypes(include=np.number).corr(), annot=True, cmap="coolwarm")
plt.tight_layout()
plt.savefig("heatmap.png")

# 3️⃣ Data Preprocessing
df = df.drop(columns=["title", "text", "url", "date_published"])  # Remove high-cardinality text columns
df = df.dropna()  # Drop rows with missing values

# Feature and Target Definition
X = df.drop(columns=["shares"])  # Features
y = df["shares"]                # Target

categorical_features = ["author", "source"]
numerical_features = ["likes", "comments"]

# Preprocessing pipeline
preprocessor = ColumnTransformer(transformers=[
    ("num", Pipeline([
        ("imputer", SimpleImputer(strategy="mean")),
        ("scaler", StandardScaler())
    ]), numerical_features),
    ("cat", Pipeline([
        ("imputer", SimpleImputer(strategy="most_frequent")),
        ("onehot", OneHotEncoder(handle_unknown="ignore"))
    ]), categorical_features)
])

# 4️⃣ Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 5️⃣ Train Random Forest Model
model = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("regressor", RandomForestRegressor(
        n_estimators=100,
        max_depth=10,
        min_samples_split=5,
        random_state=42
    ))
])

model.fit(X_train, y_train)

# 6️⃣ Evaluate Model Performance
y_pred = model.predict(X_test)

r2 = r2_score(y_test, y_pred)
mse = mean_squared_error(y_test, y_pred)
rmse = np.sqrt(mse)

print(f"\nModel Evaluation:")
print(f"R² Score : {r2:.4f}")
print(f"MSE      : {mse:.2f}")
print(f"RMSE     : {rmse:.2f}")

# 7️⃣ Feature Importance Analysis
regressor = model.named_steps["regressor"]
onehot_features = list(model.named_steps["preprocessor"]
                       .named_transformers_["cat"]
                       .named_steps["onehot"]
                       .get_feature_names_out(categorical_features))
all_features = numerical_features + onehot_features
importances = regressor.feature_importances_

# Plot Feature Importances
plt.figure(figsize=(12, 6))
sns.barplot(x=importances, y=all_features)
plt.title("Feature Importances")
plt.tight_layout()
plt.savefig("feature_importances.png")

# 8️⃣ Make Predictions (optional input simulation)
sample_input = pd.DataFrame([{
    "likes": 1200,
    "comments": 300,
    "author": "John Doe",
    "source": "dailybuzz.com"
}])

sample_prediction = model.predict(sample_input)
print("\nSample Prediction (shares):", int(sample_prediction[0]))
